{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scs1NLZfp-1L",
        "outputId": "66b4703a-74b6-4898-cf75-0881947f0c19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ERROR: unknown command \"instll\" - maybe you meant \"install\"\n"
          ]
        }
      ],
      "source": [
        "!pip instll nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ],
      "metadata": {
        "id": "fzrFCF5mqDu_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_zgS3oaqQzi",
        "outputId": "7cd3fa55-1ac3-4f57-89da-f6c4823f34ee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def wordnet_tagger(tag):\n",
        "  if tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  elif tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:\n",
        "    return wordnet.NOUN"
      ],
      "metadata": {
        "id": "TzwcM20KqbfQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "text = \"The quick brown fox jumps over the lazy dog. She is running in the park and playing with a ball on the grass. They are very happy because it is a beautiful day. I saw them while walking to the store, but I did not stop to say hello.\"\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "for sentence in sentences:\n",
        "  words = word_tokenize(sentence)\n",
        "  tagged_words = nltk.pos_tag(words)\n",
        "  for token,tag in tagged_words:\n",
        "    if token not in stop_words:\n",
        "      lemma = lemmatizer.lemmatize(token,wordnet_tagger(tag))\n",
        "      print(lemma,end=\" \")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT4Np-QKqsSC",
        "outputId": "101dc98f-9e93-4cc2-f6de-53ae4649107c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The \n",
            "quick \n",
            "brown \n",
            "fox \n",
            "jump \n",
            "\n",
            "\n",
            "lazy \n",
            "dog \n",
            ". \n",
            "She \n",
            "\n",
            "run \n",
            "\n",
            "\n",
            "park \n",
            "\n",
            "play \n",
            "\n",
            "\n",
            "ball \n",
            "\n",
            "\n",
            "grass \n",
            ". \n",
            "They \n",
            "\n",
            "\n",
            "happy \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "beautiful \n",
            "day \n",
            ". \n",
            "I \n",
            "saw \n",
            "\n",
            "\n",
            "walk \n",
            "\n",
            "\n",
            "store \n",
            ", \n",
            "\n",
            "I \n",
            "\n",
            "\n",
            "stop \n",
            "\n",
            "say \n",
            "hello \n",
            ". \n"
          ]
        }
      ]
    }
  ]
}